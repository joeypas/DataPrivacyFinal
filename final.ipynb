{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "6ec15035",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\n# Some useful utilities\n\ndef laplace_mech(v, sensitivity, epsilon):\n    return v + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n\ndef gaussian_mech(v, sensitivity, epsilon, delta):\n    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n\ndef gaussian_mech_vec(v, sensitivity, epsilon, delta):\n    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon, size=len(v))\n\ndef pct_error(orig, priv):\n    return np.abs(orig - priv)/orig * 100.0\n\ndef z_clip(xs, b):\n    return [min(x, b) for x in xs]\n\ndef g_clip(v):\n    n = np.linalg.norm(v, ord=2)\n    if n > 1:\n        return v / n\n    else:\n        return v"
        },
        {
            "cell_type": "markdown",
            "id": "a3aeb9ad",
            "metadata": {},
            "source": "# Setup\nHere we want to load our dataset, preprocessing, and split into train and test for our model"
        },
        {
            "cell_type": "markdown",
            "id": "a5c81ca7",
            "metadata": {},
            "source": "## Step 1: load the data"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "24e53584",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "default_of_credit_clients = pd.read_csv(\"default_of_credit_card_clients.csv\")\n\ncols = default_of_credit_clients.iloc[0].tolist()\ncols[-1] = \"default\"\n\ndf = default_of_credit_clients[1:].copy()\ndf.columns = cols\n\ndf = df.apply(pd.to_numeric, errors=\"coerce\")"
        },
        {
            "cell_type": "markdown",
            "id": "a12bbe9e",
            "metadata": {},
            "source": "## Step 2: Split train/test data"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "cdaa25dc",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "from sklearn.model_selection import train_test_split\n\nX = df.drop(columns=[\"default\"]).reset_index(drop=True)\ny = df[\"default\"].astype(int)\n\nassert isinstance(X, pd.DataFrame)\nassert isinstance(y, pd.Series)\n\nX = X.reset_index(drop=True)\ny = y.reset_index(drop=True)\n\ntraining_size = int(X.shape[0] * 0.8)\n\nX_train = X.iloc[:training_size]\nX_test = X.iloc[training_size:]\n\ny_train = y.iloc[:training_size]\ny_test = y.iloc[training_size:]"
        },
        {
            "cell_type": "markdown",
            "id": "bf26a844",
            "metadata": {},
            "source": "## Step 3: Convert X_train/X_test to StandardScaler"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "8b4d60d1",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.fit_transform(X_test)"
        },
        {
            "cell_type": "markdown",
            "id": "99d728fb",
            "metadata": {},
            "source": "## Step 4: Convert to numpy"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "d6f46c68",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "X_train = X_train_scaled.astype(float)\nX_test = X_test_scaled.astype(float)\n\ny_train = y_train.to_numpy().astype(float)\ny_test = y_test.to_numpy().astype(float)\n\n#fix train labels\ny_train = 2 * y_train - 1\ny_test = 2 * y_test - 1"
        },
        {
            "cell_type": "markdown",
            "id": "3de347a3",
            "metadata": {},
            "source": "# Using Scikit-Learn\nThis is going to be our baseline model that we want to compare against a \ndifferentially private gradient descent model"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "1ca73f84",
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([ 1., -1., -1., ...,  1., -1., -1.], shape=(6000,))"
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(max_iter=20000).fit(X_train, y_train)\nmodel.predict(X_test)"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "3b1be71b",
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "np.float64(0.821)"
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "np.sum(model.predict(X_test) == y_test)/X_test.shape[0]"
        },
        {
            "cell_type": "markdown",
            "id": "061349f4",
            "metadata": {},
            "source": "# Model Prediction"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "c81b7ebc",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "theta = np.zeros(X_train.shape[1])\n\ndef predict(xi, theta, bias=0):\n    label = np.sign(xi @ theta + bias)\n    return label\n\ndef accuracy(theta):\n    return np.sum(predict(X_test, theta) == y_test)/X_test.shape[0]"
        },
        {
            "cell_type": "markdown",
            "id": "172e428b",
            "metadata": {},
            "source": "# Gradient Descent Model"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "47932bde",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "def loss(theta, xi, yi):\n    exponent = - yi * (xi.dot(theta))\n    return np.log(1 + np.exp(exponent))"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "0c3e644b",
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "np.float64(0.6931471805599454)"
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "np.mean([loss(theta, x_i, y_i) for x_i, y_i in zip(X_test, y_test)])"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "e6e26f04",
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Training loss: 0.6583993849702446\nTesting loss: 0.6567817457080667\n\nTraining loss: 0.655152203542232\nTesting loss: 0.6534175479529754\n\nTraining loss: 0.6531685321073202\nTesting loss: 0.6513885900781361\n\nTraining loss: 0.6518058565795691\nTesting loss: 0.6499905636534513\n\nTraining loss: 0.6508191961271063\nTesting loss: 0.6489932882115115\n\nTraining loss: 0.6500873136521533\nTesting loss: 0.6482481660445689\n\nTraining loss: 0.6495359785460764\nTesting loss: 0.6476933153126854\n\nTraining loss: 0.6491157849005481\nTesting loss: 0.6472706358573627\n\nTraining loss: 0.6487923316131161\nTesting loss: 0.6469489119182625\n\nTraining loss: 0.6485410940842373\nTesting loss: 0.6467011637909764\n\nTraining loss: 0.648344301400021\nTesting loss: 0.6465101152796263\n\nTraining loss: 0.6481889167491821\nTesting loss: 0.6463619651079584\n\nTraining loss: 0.6480652762264731\nTesting loss: 0.6462469625753956\n\nTraining loss: 0.6479661510255315\nTesting loss: 0.6461575501060008\n\nTraining loss: 0.6478860896568733\nTesting loss: 0.6460881076457649\n\nTraining loss: 0.6478209503429966\nTesting loss: 0.6460342943049177\n\nTraining loss: 0.6477675646892262\nTesting loss: 0.6459927903830578\n\nTraining loss: 0.6477234933324493\nTesting loss: 0.6459610139672145\n\nTraining loss: 0.6476868468073473\nTesting loss: 0.6459369556122027\n\nTraining loss: 0.647656153154352\nTesting loss: 0.6459190372821024\n\nTraining loss: 0.6476302593451313\nTesting loss: 0.6459060152591424\n\nTraining loss: 0.6476082573853292\nTesting loss: 0.6458969035525411\n\nTraining loss: 0.647589428565514\nTesting loss: 0.6458909174029503\n\nTraining loss: 0.6475732011546058\nTesting loss: 0.6458874295074802\n\nTraining loss: 0.6475591181162977\nTesting loss: 0.645885936682872\n\nTraining loss: 0.6475468123448823\nTesting loss: 0.6458860339769118\n\nTraining loss: 0.6475359875746893\nTesting loss: 0.6458873945934612\n\nTraining loss: 0.6475264035935131\nTesting loss: 0.6458897541747578\n\nTraining loss: 0.6475178647376156\nTesting loss: 0.6458928984569401\n\nTraining loss: 0.6475102109007932\nTesting loss: 0.6458966535143149\n\nTraining loss: 0.6475033104783126\nTesting loss: 0.6459008780175916\n\nTraining loss: 0.6474970548064656\nTesting loss: 0.6459054570604496\n\nTraining loss: 0.6474913537630654\nTesting loss: 0.645910297216632\n\nTraining loss: 0.6474861322727478\nTesting loss: 0.6459153225660296\n\nTraining loss: 0.6474813275202239\nTesting loss: 0.6459204714879393\n\nTraining loss: 0.6474768867195763\nTesting loss: 0.6459256940642285\n\nTraining loss: 0.647472765321915\nTesting loss: 0.6459309499695628\n\nTraining loss: 0.6474689255698854\nTesting loss: 0.6459362067521524\n\nTraining loss: 0.647465335327605\nTesting loss: 0.6459414384288479\n\nTraining loss: 0.6474619671300895\nTesting loss: 0.6459466243342069\n\nTraining loss: 0.647458797408204\nTesting loss: 0.6459517481754625\n\nTraining loss: 0.6474558058544596\nTesting loss: 0.6459567972549785\n\nTraining loss: 0.6474529749022196\nTesting loss: 0.6459617618293583\n\nTraining loss: 0.6474502892965296\nTesting loss: 0.6459666345803744\n\nTraining loss: 0.6474477357392189\nTesting loss: 0.6459714101776405\n\nTraining loss: 0.6474453025944086\nTesting loss: 0.6459760849167383\n\nTraining loss: 0.647442979643308\nTesting loss: 0.6459806564195442\n\nTraining loss: 0.6474407578793644\nTesting loss: 0.6459851233859325\n\nTraining loss: 0.6474386293365516\nTesting loss: 0.6459894853879938\n\nTraining loss: 0.647436586944963\nTesting loss: 0.6459937426994949\n\nTraining loss: 0.6474346244089741\nTesting loss: 0.6459978961545884\n\nTraining loss: 0.6474327361041166\nTesting loss: 0.6460019470308344\n\nTraining loss: 0.6474309169895163\nTesting loss: 0.6460058969524469\n\nTraining loss: 0.6474291625333136\nTesting loss: 0.6460097478103791\n\nTraining loss: 0.6474274686489512\nTesting loss: 0.6460135016964383\n\nTraining loss: 0.647425831640579\nTesting loss: 0.6460171608490958\n\nTraining loss: 0.6474242481561377\nTesting loss: 0.6460207276090426\n\nTraining loss: 0.6474227151469225\nTesting loss: 0.6460242043828723\n\nTraining loss: 0.6474212298326348\nTesting loss: 0.6460275936135361\n\nTraining loss: 0.6474197896710945\nTesting loss: 0.6460308977564382\n\nTraining loss: 0.6474183923319201\nTesting loss: 0.6460341192602258\n\nTraining loss: 0.6474170356735964\nTesting loss: 0.6460372605514836\n\nTraining loss: 0.6474157177234432\nTesting loss: 0.6460403240226684\n\nTraining loss: 0.647414436660074\nTesting loss: 0.6460433120227301\n\nTraining loss: 0.6474131907979979\nTesting loss: 0.6460462268499543\n\nTraining loss: 0.6474119785740706\nTesting loss: 0.6460490707466364\n\nTraining loss: 0.6474107985355446\nTesting loss: 0.6460518458952652\n\nTraining loss: 0.6474096493295071\nTesting loss: 0.6460545544159367\n\nTraining loss: 0.6474085296935227\nTesting loss: 0.6460571983647783\n\nTraining loss: 0.6474074384473266\nTesting loss: 0.6460597797331856\n\nTraining loss: 0.6474063744854351\nTesting loss: 0.6460623004477185\n\nTraining loss: 0.647405336770558\nTesting loss: 0.6460647623705201\n\nTraining loss: 0.6474043243277148\nTesting loss: 0.6460671673001512\n\nTraining loss: 0.6474033362389704\nTesting loss: 0.6460695169727471\n\nTraining loss: 0.6474023716387128\nTesting loss: 0.6460718130634212\n\nTraining loss: 0.6474014297094146\nTesting loss: 0.6460740571878538\n\nTraining loss: 0.6474005096778159\nTesting loss: 0.6460762509040141\n\nTraining loss: 0.6473996108114858\nTesting loss: 0.646078395713972\n\nTraining loss: 0.6473987324157162\nTesting loss: 0.6460804930657674\n\nTraining loss: 0.6473978738307122\nTesting loss: 0.6460825443553089\n\nTraining loss: 0.6473970344290475\nTesting loss: 0.646084550928276\n\nTraining loss: 0.647396213613354\nTesting loss: 0.6460865140820133\n\nTraining loss: 0.6473954108142222\nTesting loss: 0.646088435067396\n\nTraining loss: 0.6473946254882905\nTesting loss: 0.6460903150906605\n\nTraining loss: 0.6473938571165013\nTesting loss: 0.6460921553151896\n\nTraining loss: 0.6473931052025099\nTesting loss: 0.6460939568632457\n\nTraining loss: 0.6473923692712286\nTesting loss: 0.6460957208176498\n\nTraining loss: 0.647391648867493\nTesting loss: 0.6460974482234009\n\nTraining loss: 0.6473909435548386\nTesting loss: 0.6460991400892351\n\nTraining loss: 0.6473902529143777\nTesting loss: 0.6461007973891243\n\nTraining loss: 0.6473895765437643\nTesting loss: 0.646102421063712\n\nTraining loss: 0.6473889140562417\nTesting loss: 0.6461040120216901\n\nTraining loss: 0.6473882650797641\nTesting loss: 0.6461055711411143\n\nTraining loss: 0.6473876292561835\nTesting loss: 0.6461070992706622\n\nTraining loss: 0.6473870062404986\nTesting loss: 0.646108597230834\n\nTraining loss: 0.647386395700158\nTesting loss: 0.6461100658150974\n\nTraining loss: 0.6473857973144147\nTesting loss: 0.6461115057909822\n\nTraining loss: 0.6473852107737254\nTesting loss: 0.6461129179011202\n\nTraining loss: 0.6473846357791921\nTesting loss: 0.6461143028642394\n\nTraining loss: 0.6473840720420423\nTesting loss: 0.6461156613761097\n\n"
                },
                {
                    "data": {
                        "text/plain": "np.float64(0.5786666666666667)"
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "def logistic(x):\n    return 1 / (1 + np.exp(-x))\n\n\ndef gradient(theta, xi, yi):\n    z = yi * np.dot(xi, theta)\n\n    if z >= 0:\n        exp_neg_z = np.exp(-z)\n        sigma = 1 / (1 + exp_neg_z)\n    else:\n        exp_z = np.exp(z)\n        sigma = exp_z / (1 + exp_z)\n\n    return -yi * xi * (1 - sigma)\n\ndef avg_grad(theta, X, y):\n    grads = [gradient(theta, xi, yi) for xi, yi in zip(X, y)]\n    return np.mean(grads, axis=0)\n\ndef gradient_descent(iterations):\n    theta = np.zeros(X_train.shape[1])\n\n    for _ in range(iterations):\n        theta = theta - avg_grad(theta, X_train, y_train)\n        print(f'Training loss: {np.mean(loss(theta, X_train, y_train))}')\n        print(f'Testing loss: {np.mean(loss(theta, X_test, y_test))}\\n')\n    return theta\n\ntheta = gradient_descent(10)\naccuracy(theta)"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}